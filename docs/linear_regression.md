##### 监督学习  supervised learning   
一、数据集的标记方法  
(1)、x^(i)^表示输入变量(input variable)，也叫输入特征(input features)  
(2)、y^(i)^表示输出或目标变量(output or target vavialbe)，也是机器学习需要预测的目标;  
(3)、a pair of (x^(i)^,y^(i)^)称为一个训练样本,m个这样的样本组成一个训练集  

二、监督学习  
监督学习比较正式的定义是，给定一个训练集，学习到一个函数h：  
```math
\chi  \mapsto  y
```
h(x)是对对应值y的一个很好的估计,由于历史存在的问题，将这个函数h成为假设(hypothesis)。  
如果我们估计的值是个连续的值，我们将我们的学习问题称为回归，如果估计的值是一个离散的值，我们将我们的学习问题成为分类。  

##### 监督学习  线性回归  
一个实例：一个地方的房屋价格和居住面积、卧室个数的数据如下表所示：

居住面积(平米) | 卧室个数 | 价格
---|---|---
2104 | 3 | 400
1600 | 3 | 330
2400 | 3 | 369
1416 | 2 | 232
3000 | 4 | 540
...|...|...  

在上面的实例中，x是两个维度的向量，第一个维度是房屋面积，第二维度是卧室个数。在设计一个学习问题的时候，需要选择什么样的特征是由设计者决定的，你还可以选择房屋的洗澡间的个数等。  
去做监督学习，我们需要确定在计算机里边怎样去表示这个函数/假设，作为一个初始的选择，我们假设用一个x的线性函数来估计y的值： 

```math
h_{\theta}(x) = \theta_{0} + \theta_{1}x_{1}+\theta_{2}x_{2}
```
上述中 `$\theta_{i}$`为参数(也称为权重)，为了进一步简化我们的表示，我们假设 `$x_0=1$`,因此上式又可以写成：  

```math
h_{\theta}(x)=\sum_{i=0}^{n}\theta_ix_i=\theta^Tx
```
上述中`$\theta$`和`$x$`都是向量表示。  
给定一个数据集的情况下，怎样选择参数`$\theta$`，使得假设能够比较接近y呢？这里，我们引入了成本函数(cost function)，它衡量了假设和y值得接近程度，即最小平方(最小二乘法)成本函数(least square cost function)  

```math
J(\theta)= \frac{1}{2}\sum_{i=0}^{m}(h_{\theta}(x^{(i)}-y^{(i)})^2
```


##### 最小均方算法(LMS, Least Mean Square)  
我们的目标是选择合适的`$\theta$`去最小化成本函数`$J(\theta)$`。我们从一个猜想的初始`$\theta$`开始，逐步去更新`$\theta$`，使得`$J(\theta)$`更小，最终收敛到一个`$\theta$`值，使得`$J(\theta)$`最小。  
  
更加具体一点的是，考虑梯度下降算法(gradient descent)，从一个初始的`$\theta$`值开始，不断重复地去做如下的更新：  

```math
\theta_{j} := \theta_{j}-\alpha\frac{\partial}{\partial\theta_{j}}J(\theta)
```
上述的更新针对所有的`$j=0,1,2...n$`，这里`$\alpha$`是学习率，在这里每一步选择`$J(\theta)$`的最陡的方向进行`$\theta$`的更新。  

当只有一个样本的时候，有如下关系成立：

```math
\frac{\partial}{\theta_{i}}J(\theta)=\frac{\partial}{\partial\theta_{i}}\frac{1}{2}(h_{\theta}(x)-y)^2

=2\cdot \frac{1}{2}(h_{\theta}(x)-y)\cdot \frac{\partial}{\partial \theta_{i}}(h_{\theta(x)}-y)

=(h_{\theta(x)}-y)\cdot x_{i}

```
因此，当只有一个样本的时候，`$\theta$`的更新规则如下：

```math
\theta_{j} := \theta_{j}+\alpha(y^{(i)}-h_\theta(x^{(i)})x_j^{(i)}
```
这个规则也叫做最小均方更新规则，这个规则看上去很自然也很直观，比如更新的幅度正比于error项`$y^{(i)}-h_\theta(x^{(i)}$`，当预测的值和y值比较match的时候，误差项比较小，`$\theta$`更新的幅度也小，当预测的值和y值相差比较大的时候，说明模型参数还没有能很好地进行目标预测，`$\theta$`更新的幅度也大。

当有多个样本的时候，有两种方法可以对上述规则进行扩展，第一种方法称为批量梯度下降法(batch gradient decent),具体更新规则如下：  
迭代如下公式直至收敛
```math
\theta_{j} := \theta_{j}+\alpha \sum_{i=1}^{m}(y^{(i)}-h_\theta(x^{(i)})x_j^{(i)}\  (for \  every \ j)
```
需要注意的是，梯度下降法容易找到局部最小点，但是在这里提出的线性回归的优化问题只有一个全局的，而没有局部的最优值。因此在这里梯度下降法总是收敛到全局的最小值。在这里`$J$`是一个凸的二次函数，可以用梯度下降法去优化凸二次函数。  
第二种方法称为随机梯度下降法(stochastic gradient decent),具体更新规则如下：  
迭代如下公式：  

```math
for\ i=1\ to\ m: \{ 
\theta_{j} := \theta_{j}+\alpha (y^{(i)}-h_\theta(x^{(i)})x_j^{(i)}\  (for \  every \ j)\}
```
在随机梯度下降的过程中，我们每次碰到一个样本，`$\theta$`更新的时候只针对当前的样本去算梯度。批量梯度下降是扫描整个数据集然后更新参数，这是一个非常耗费的运算，而随机梯度更新一次参数的运算只针对单个样本，计算很快，所以能更快的接近最值点。需要注意的是，随机梯度下降不能达到最值点，在最后会在最值点震荡，但在很多问题中这样的优化策略已经足够实用。因此在实际场景中，随机梯度下降法比批量梯度下降法更实用。














